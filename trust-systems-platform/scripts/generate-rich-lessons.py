#!/usr/bin/env python3
"""
Generate rich lesson content for W03-W24 (110 files).
Transforms vague stubs into Day-1-quality lessons (~200+ lines each).

Usage: python3 scripts/generate-rich-lessons.py
"""

import os
import re
import sys
import yaml

BASE_DIR = os.path.join(os.path.dirname(__file__), '..', 'content', 'trust_platform_content', 'parts')


# â”€â”€â”€ TEMPLATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_lesson(d):
    """Generate full Day-1-quality markdown from lesson data dict."""
    fm = d['frontmatter']
    prereqs_str = ""
    if fm.get('prereqs'):
        prereqs_str = "\n".join(f'  - "{p}"' for p in fm['prereqs'])
        prereqs_str = f"prereqs:\n{prereqs_str}"
    else:
        prereqs_str = "prereqs: []"

    deliverables = "\n".join(f"- âœ… {x}" for x in d['goal_deliverables'])
    pass_rows = "\n".join(f"| {i+1} | {c[0]} | {c[1]} |" for i, c in enumerate(d['pass_criteria']))
    build_deliverables = "\n".join(f"- âœ… {x}" for x in d['build_deliverables'])

    without = "\n".join(f"- {x}" for x in d['why_without'])
    with_ = "\n".join(f"- {x}" for x in d['why_with'])
    connects = "\n".join(f"- {x}" for x in d['why_connects'])

    do_steps = ""
    for i, step in enumerate(d['do_steps']):
        do_steps += f"\n{i+1}. **{step['title']}**\n"
        do_steps += f"   > ğŸ’¡ *WHY: {step['why']}*\n"
        if step.get('content'):
            do_steps += f"\n{step['content']}\n"

    done_when = "\n".join(f"- [ ] {x}" for x in d['done_when'])
    self_test = "\n".join(f"{i+1}. {q[0]} â†’ **{q[1]}**" for i, q in enumerate(d['self_test']))

    return f"""---
id: {fm['id']}
part: {fm['part']}
title: "{fm['title']}"
order: {fm['order']}
duration_minutes: 120
{prereqs_str}
proof:
  type: "paste_or_upload"
  status: "manual_or_regex"
review_schedule_days: [3,7,21,60]
---

# {fm['title']}

## Goal

{d['goal_intro']}

By end of this session you will have:

{deliverables}

**PASS CRITERIA** (must achieve ALL):

| # | Criterion | How to check |
|---|-----------|-------------|
{pass_rows}

## What You're Building Today

{d['build_description']}

By end of this session, you will have:

{build_deliverables}

What "done" looks like:

```{d.get('code_lang', 'markdown')}
{d['done_example']}
```

You **can**: {d['can_do']}
You **cannot yet**: {d['cannot_yet']}

## Why This Matters

ğŸ”´ **Without this, you will:**
{without}

ğŸŸ¢ **With this, you will:**
{with_}

ğŸ”— **How this connects:**
{connects}

ğŸ§  **Mental model: "{d['mental_model_name']}"**

{d['mental_model_desc']}

## Visual Model

```
{d['visual_model']}
```

## Build

File: `{d['ship_file']}`

## Do
{do_steps}
## Done when

{done_when}

## Proof

{d['proof_instruction']}

**Quick self-test** (answer without looking at your notes):
> ğŸ’¡ *WHY these questions: If you can answer all 3 instantly, you've internalized the concept. If not, re-read â€” these come back in future weeks.*

{self_test}
"""


# â”€â”€â”€ LESSON DATA: WEEK 3 â€” Multi-Client Event Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

W03_LESSONS = [
    {
        'frontmatter': {
            'id': 'w03-multi-client-event-loop-d01-quest-event-loop-state-model-2h',
            'part': 'w03-multi-client-event-loop',
            'title': 'Quest: Event Loop State Model  2h',
            'order': 1,
            'prereqs': [],
        },
        'goal_intro': 'Design the **complete state model** for a non-blocking event loop so every connection has explicit, trackable state transitions before you write any loop code.',
        'goal_deliverables': [
            'A **connection state enum** covering all lifecycle phases (connecting, reading, writing, closing)',
            'A **state transition table** showing valid transitions with trigger events',
            'A **blocking-call audit** proving no blocking operations exist in the loop path',
            'A **per-connection data structure** spec showing what each connection tracks',
        ],
        'pass_criteria': [
            ('State enum has â‰¥ 5 states', 'Count enum values'),
            ('Transition table covers all state pairs', 'Verify every state has at least one exit transition'),
            ('Blocking audit has zero violations', 'Search for recv/send/accept without O_NONBLOCK'),
            ('Per-connection struct has â‰¥ 4 fields', 'Review struct definition'),
            ('EAGAIN handling documented for every I/O call', 'Check each recv/send path'),
        ],
        'build_description': 'A design document defining the state machine for your multi-client event loop â€” the "blueprint" before writing C++ loop code.',
        'build_deliverables': [
            'File: `week-3/day1-event-loop-state-model.md`',
            'Connection state enum with 5+ states: `CONNECTING`, `READING_HEADER`, `READING_BODY`, `WRITING`, `CLOSING`',
            'State transition table with events and guard conditions',
            'Per-connection struct: `{ fd, state, read_buf, write_buf, last_active, bytes_queued }`',
        ],
        'done_example': '''## Connection States
enum ConnState {
    CONNECTING,      // accept() returned, waiting for first data
    READING_HEADER,  // partial header received, need more bytes
    READING_BODY,    // header complete, reading payload
    WRITING,         // response queued, draining write buffer
    CLOSING          // shutdown initiated, draining then close
};

## Per-Connection Data
struct Connection {
    int fd;
    ConnState state;
    std::vector<uint8_t> read_buf;
    std::vector<uint8_t> write_buf;
    time_t last_active;
    size_t bytes_queued;
};''',
        'code_lang': 'cpp',
        'can_do': 'Describe every state a connection can be in and what triggers each transition.',
        'cannot_yet': 'Run the event loop â€” that starts on Day 2 when you implement the select loop.',
        'why_without': [
            'Treat connections as stateless and lose track of partial reads mid-message',
            'Accidentally call blocking `recv()` inside the event loop, freezing all clients',
            'Have no way to detect or handle `EAGAIN` â€” the kernel telling you "not ready yet"',
            'Waste hours debugging "why does the server hang with 2+ clients?"',
        ],
        'why_with': [
            'Know exactly what phase each connection is in at all times',
            'Handle `EAGAIN` as a normal event, not an error â€” the key insight of non-blocking I/O',
            'Detect stuck connections by checking state + `last_active` timestamp',
            'Build a foundation that scales cleanly from 2 clients to 200 clients',
        ],
        'why_connects': [
            '**To Day 2:** You will plug this state model into a `select()` loop',
            '**To Day 3:** Backpressure policy uses `bytes_queued` from this struct',
            '**To Day 4:** `poll()` migration preserves these exact states',
            '**To Week 4:** `epoll` upgrade keeps the same state machine, different notification',
            '**To Week 6:** Overload detection triggers on state queue depths defined here',
        ],
        'mental_model_name': 'Explicit State Machines',
        'mental_model_desc': '''In amateur code: state is scattered across local variables, if/else chains, and implicit assumptions.
In professional systems: **every entity has an explicit state enum with documented transitions.**

Event loops fail when state is implicit. A connection that\'s "reading" but also has unsent data in its write buffer
is in an ambiguous state. Your state model eliminates ambiguity.

By Week 11 when building replication, each follower node will have an explicit state machine too.
This habit starts TODAY â€” make state visible, make transitions explicit.''',
        'visual_model': '''â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               CONNECTION STATE MACHINE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚   accept()                                               â”‚
â”‚      â”‚                                                   â”‚
â”‚      â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  readable    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ CONNECTING  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ READING_HEADER   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                       â”‚ header complete  â”‚
â”‚                                       â–¼                  â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚                              â”‚ READING_BODY     â”‚        â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                       â”‚ body complete    â”‚
â”‚                                       â–¼                  â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚       EAGAIN â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ WRITING          â”‚        â”‚
â”‚       (retry later)          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                       â”‚ write complete   â”‚
â”‚                                       â–¼                  â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚                              â”‚ CLOSING           â”‚        â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                       â”‚ fd closed        â”‚
â”‚                                       â–¼                  â”‚
â”‚                                   [removed]              â”‚
â”‚                                                          â”‚
â”‚  ANY STATE â”€â”€â”€â”€ error/timeout â”€â”€â”€â–¶ CLOSING               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜''',
        'ship_file': 'week-3/day1-event-loop-state-model.md',
        'do_steps': [
            {
                'title': 'Define the connection state enum',
                'why': 'Every connection needs a finite set of named states. Without this, you\'ll use booleans like `is_reading` and `is_writing` that create impossible combinations.',
                'content': '''   Define at least 5 states. For each state, write one sentence explaining what the connection is doing:

   ```cpp
   // State: CONNECTING
   // Connection accepted but no data exchanged yet.
   // Valid next: READING_HEADER (on readable), CLOSING (on timeout)
   ```''',
            },
            {
                'title': 'Build the state transition table',
                'why': 'A transition table prevents illegal state changes. On Day 4 when you migrate to `poll()`, this table is your regression checklist â€” behavior must be identical.',
                'content': '''   Create a table with columns: Current State â†’ Event â†’ Next State â†’ Action:

   | Current | Event | Next | Action |
   |---------|-------|------|--------|
   | CONNECTING | fd readable | READING_HEADER | Start reading into buffer |
   | READING_HEADER | header complete | READING_BODY | Parse header, continue reading |
   | READING_BODY | body complete | WRITING | Prepare response, queue write |
   | WRITING | write complete | CLOSING | Initiate graceful shutdown |
   | WRITING | EAGAIN | WRITING | Register for writability, retry later |
   | ANY | error | CLOSING | Log error, begin close sequence |
   | ANY | timeout | CLOSING | Log timeout, begin close sequence |''',
            },
            {
                'title': 'Design the per-connection data structure',
                'why': 'Each connection is independent state. The struct IS the connection\'s memory. In Week 5 when threads access connections, ownership of this struct prevents data races.',
                'content': '''   Define a struct with these minimum fields:

   ```cpp
   struct Connection {
       int fd;                          // socket file descriptor
       ConnState state;                 // current state enum value
       std::vector<uint8_t> read_buf;   // partial read accumulator
       std::vector<uint8_t> write_buf;  // pending write data
       time_t last_active;              // for timeout detection
       size_t bytes_queued;             // for backpressure tracking
   };
   ```

   Add any additional fields you need for your protocol (e.g., `request_id`, `frame_bytes_remaining`).''',
            },
            {
                'title': 'Audit for blocking calls',
                'why': 'ONE blocking call inside the event loop freezes ALL clients. This audit catches the #1 event-loop bug before it happens.',
                'content': '''   Search your planned code paths for these dangerous patterns:

   | Call | Danger | Non-blocking alternative |
   |------|--------|------------------------|
   | `recv(fd, ...)` without `O_NONBLOCK` | Blocks if no data ready | Set `O_NONBLOCK` on fd after `accept()` |
   | `send(fd, ...)` without `O_NONBLOCK` | Blocks if send buffer full | Check writability first, handle `EAGAIN` |
   | `accept()` without `O_NONBLOCK` | Blocks if no pending conn | Set `O_NONBLOCK` on listen socket |
   | `sleep()` / `usleep()` | Freezes entire loop | Use poll/select timeout instead |

   Document your rule: **"Every fd is set `O_NONBLOCK` immediately after creation."**''',
            },
            {
                'title': 'Define EAGAIN handling for every I/O path',
                'why': '`EAGAIN` is not an error â€” it means "try again later." Mishandling it causes silent data loss. By Week 6, your backpressure system depends on correct EAGAIN response.',
                'content': '''   For each I/O operation, define what happens on `EAGAIN`:

   ```
   recv() returns EAGAIN:
     â†’ Keep current state
     â†’ Re-register fd for read readiness
     â†’ Do NOT close connection
     â†’ Do NOT discard partial buffer

   send() returns EAGAIN:
     â†’ Keep remaining data in write_buf
     â†’ Register fd for write readiness
     â†’ Transition state to WRITING (if not already)
     â†’ bytes_queued tracks unsent amount
   ```

   **Rule:** EAGAIN means "not ready yet" â€” NEVER means "failed."''',
            },
        ],
        'done_when': [
            'Connection state enum with 5+ named states â€” *becomes your Day 2 switch/case structure*',
            'State transition table with events and guard conditions â€” *your Day 4 regression checklist*',
            'Per-connection struct with fd, state, buffers, timestamp, queue counter â€” *the data each connection owns*',
            'Blocking call audit with zero violations â€” *prevents the #1 event-loop bug*',
            'EAGAIN handling documented for recv and send â€” *the foundation of non-blocking I/O*',
        ],
        'proof_instruction': 'Paste your connection state enum, transition table, and per-connection struct, or upload `week-3/day1-event-loop-state-model.md`.',
        'self_test': [
            ('What does `EAGAIN` mean when `recv()` returns it?', '"Not ready yet â€” try again later." It is NOT an error.'),
            ('What happens if you call blocking `recv()` inside an event loop with 50 clients?', 'The entire loop freezes until that one recv completes â€” all 49 other clients wait.'),
            ('What state must a connection track to implement backpressure?', '`bytes_queued` (or write buffer size) â€” if it exceeds a threshold, stop reading from that client.'),
        ],
    },
    {
        'frontmatter': {
            'id': 'w03-multi-client-event-loop-d02-quest-first-multi-client-loop-2h',
            'part': 'w03-multi-client-event-loop',
            'title': 'Quest: First Multi-Client Loop  2h',
            'order': 2,
            'prereqs': ['w03-multi-client-event-loop-d01-quest-event-loop-state-model-2h'],
        },
        'goal_intro': 'Plan and design your first **multi-client event loop** using `select()` so you can serve 50+ concurrent clients from a single thread without blocking.',
        'goal_deliverables': [
            'A **select() loop pseudocode** showing fd_set setup, timeout, and dispatch',
            'A **connection registry** design for tracking all active connections',
            'A **connect/disconnect burst test plan** for 50+ clients',
            'A **max-fd tracking strategy** to handle `select()` limits correctly',
        ],
        'pass_criteria': [
            ('select() loop pseudocode handles read, write, and error sets', 'Verify all 3 fd_sets used'),
            ('Connection registry supports add/remove/lookup by fd', 'Check API signatures'),
            ('Max-fd tracking updates on every add/remove', 'Trace through add/remove paths'),
            ('Test plan covers 50+ concurrent idle clients', 'Count test scenario rows'),
            ('Timeout value chosen and justified', 'Look for timeout rationale'),
        ],
        'build_description': 'A design document for your first multi-client server using `select()` â€” the simplest multiplexing API that works everywhere.',
        'build_deliverables': [
            'File: `week-3/day2-select-plan.md`',
            'select() loop pseudocode with fd_set setup and event dispatch',
            'Connection registry API: `add(fd)`, `remove(fd)`, `get(fd)`',
            'Test plan for 50+ concurrent connections with connect/disconnect bursts',
        ],
        'done_example': '''## Select Loop Pseudocode
while (running) {
    fd_set read_fds, write_fds, error_fds;
    FD_ZERO(&read_fds); FD_ZERO(&write_fds); FD_ZERO(&error_fds);

    FD_SET(listen_fd, &read_fds);  // always watch for new connections
    int max_fd = listen_fd;

    for (auto& [fd, conn] : registry) {
        FD_SET(fd, &read_fds);     // always watch for data
        if (!conn.write_buf.empty())
            FD_SET(fd, &write_fds); // watch for writability if data pending
        FD_SET(fd, &error_fds);
        max_fd = std::max(max_fd, fd);
    }

    struct timeval tv = {.tv_sec = 1, .tv_usec = 0};
    int ready = select(max_fd + 1, &read_fds, &write_fds, &error_fds, &tv);
    // dispatch to handlers...
}''',
        'code_lang': 'cpp',
        'can_do': 'Design a complete multi-client loop that tracks and dispatches events for 50+ connections.',
        'cannot_yet': 'Handle backpressure â€” that comes on Day 3. You also can\'t handle `poll()` migration yet (Day 4).',
        'why_without': [
            'Be limited to one client at a time (blocking server) â€” useless for real workloads',
            'Misuse `select()` by forgetting to rebuild fd_sets each iteration (they\'re destructive)',
            'Forget to track `max_fd` and silently miss events on higher-numbered descriptors',
            'Have no plan for handling connect/disconnect storms that happen in production',
        ],
        'why_with': [
            'Serve 50+ clients from a single thread â€” the foundation of all high-performance servers',
            'Understand `select()` mechanics deeply before migrating to better APIs',
            'Have a clear connection registry that maps fd â†’ state for instant lookups',
            'Have a test plan that proves your loop survives connection churn',
        ],
        'why_connects': [
            '**To Day 1:** Your state model plugs directly into this loop\'s dispatch logic',
            '**To Day 3:** Backpressure policy adds write-buffer limits to this registry',
            '**To Day 4:** `poll()` replaces `select()` but your dispatch logic stays identical',
            '**To Week 4:** `epoll` replaces `poll()` â€” same pattern, better scaling',
            '**To Week 5:** Thread pool offloads CPU work FROM this loop',
        ],
        'mental_model_name': 'Event-Driven Dispatch',
        'mental_model_desc': '''In blocking servers: one thread per client. 1000 clients = 1000 threads = memory explosion.
In event-driven servers: **one thread, one loop, N clients. The loop asks "who\'s ready?" and dispatches.**

`select()` is the simplest form of this pattern. It\'s limited (~1024 fds) but teaches the core concept:
**readiness notification â†’ dispatch â†’ handle â†’ back to waiting.**

Every high-performance server (nginx, Redis, Node.js) uses this pattern. You\'re learning the primitive version first.''',
        'visual_model': '''â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SELECT() EVENT LOOP                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚ 1. Build fd_sets from registry   â”‚                    â”‚
â”‚  â”‚    read_fds: all connections     â”‚                    â”‚
â”‚  â”‚    write_fds: those with data    â”‚                    â”‚
â”‚  â”‚    error_fds: all connections    â”‚                    â”‚
â”‚  â”‚    max_fd: track highest fd      â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                 â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚ 2. select(max_fd+1, r, w, e, t) â”‚ â—€â”€â”€ blocks here    â”‚
â”‚  â”‚    (waits for readiness)          â”‚     until ready    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                 â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚ 3. Dispatch ready fds:           â”‚                    â”‚
â”‚  â”‚    listen_fd â†’ accept new conn   â”‚                    â”‚
â”‚  â”‚    readable  â†’ recv into buffer  â”‚                    â”‚
â”‚  â”‚    writable  â†’ send from buffer  â”‚                    â”‚
â”‚  â”‚    error     â†’ close + cleanup   â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                 â”‚                                         â”‚
â”‚                 â””â”€â”€â”€â”€ loop back to step 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚
â”‚                                                          â”‚
â”‚  Connection Registry: fd â†’ Connection { state, bufs }    â”‚
â”‚  [fd=4: READING] [fd=7: WRITING] [fd=12: READING] ...   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜''',
        'ship_file': 'week-3/day2-select-plan.md',
        'do_steps': [
            {
                'title': 'Design the connection registry',
                'why': 'You need O(1) lookup from fd to connection state. A `std::unordered_map<int, Connection>` gives you this. On Day 3, backpressure decisions use this registry.',
                'content': '''   ```cpp
   class ConnectionRegistry {
       std::unordered_map<int, Connection> conns_;
       int max_fd_ = -1;
   public:
       void add(int fd, Connection conn);  // insert + update max_fd
       void remove(int fd);                 // erase + recalculate max_fd
       Connection* get(int fd);             // lookup, returns nullptr if missing
       int max_fd() const;                  // for select() first argument
       // Iterator for building fd_sets
       auto begin() { return conns_.begin(); }
       auto end() { return conns_.end(); }
   };
   ```''',
            },
            {
                'title': 'Write the select() loop pseudocode',
                'why': '`select()` DESTROYS the fd_sets on return â€” you must rebuild them every iteration. Forgetting this causes "server stops responding after first event."',
                'content': '''   Write the full loop structure showing:
   - fd_set initialization (FD_ZERO every iteration!)
   - Adding listen_fd to read_fds
   - Adding all client fds to appropriate sets
   - select() call with timeout
   - Dispatch logic checking FD_ISSET for each fd

   **Critical rule:** FD_ZERO + rebuild EVERY iteration. select() modifies the sets.''',
            },
            {
                'title': 'Define the accept handler',
                'why': 'New connections arrive as readability on the listen socket. You must set the new fd to `O_NONBLOCK` immediately â€” this is the rule from Day 1.',
                'content': '''   ```
   if FD_ISSET(listen_fd, &read_fds):
       new_fd = accept(listen_fd, ...)
       set_nonblocking(new_fd)           // CRITICAL: O_NONBLOCK immediately
       registry.add(new_fd, Connection{
           .fd = new_fd,
           .state = CONNECTING,
           .last_active = now()
       })
   ```''',
            },
            {
                'title': 'Define read/write dispatch handlers',
                'why': 'Read and write handlers are where partial I/O and EAGAIN happen. Your Day 1 state machine drives these transitions.',
                'content': '''   For readable fds:
   ```
   bytes = recv(fd, buf, sizeof(buf), 0)
   if bytes > 0:  update read_buf, advance state per transition table
   if bytes == 0: peer closed â†’ transition to CLOSING
   if bytes < 0 && errno == EAGAIN: do nothing, try again next loop
   if bytes < 0 && errno != EAGAIN: error â†’ transition to CLOSING
   ```

   For writable fds:
   ```
   bytes = send(fd, write_buf.data(), write_buf.size(), 0)
   if bytes > 0:  remove sent bytes from write_buf
   if bytes < 0 && errno == EAGAIN: do nothing, try again next loop
   if write_buf.empty(): all data sent â†’ advance state
   ```''',
            },
            {
                'title': 'Plan the 50-client test',
                'why': 'You need to prove this works under realistic load. 50 clients exposes fd_set rebuild bugs, max_fd tracking errors, and connection churn issues.',
                'content': '''   Design a test scenario table:

   | # | Scenario | Expected |
   |---|----------|----------|
   | 1 | 50 clients connect simultaneously | All accepted, all in registry |
   | 2 | All 50 send one message | All messages received correctly |
   | 3 | 25 clients disconnect | 25 remain, no fd leaks |
   | 4 | 25 new clients connect while 25 active | Total 50 again, no confusion |
   | 5 | All 50 send messages rapidly | No data loss, no starvation |
   | 6 | Kill server during active connections | Clean shutdown attempted |''',
            },
        ],
        'done_when': [
            'Connection registry with add/remove/lookup in O(1) â€” *plugs directly into Day 3 backpressure checks*',
            'select() loop pseudocode rebuilds fd_sets every iteration â€” *prevents the #1 select bug*',
            'Accept handler sets O_NONBLOCK immediately â€” *Day 1 rule enforced*',
            'Read/write handlers follow EAGAIN protocol â€” *no data loss on partial I/O*',
            '50-client test plan with 6+ scenarios â€” *proves loop survives real connection patterns*',
        ],
        'proof_instruction': 'Paste your select() loop pseudocode and connection registry design, or upload `week-3/day2-select-plan.md`.',
        'self_test': [
            ('Why must you call FD_ZERO before every select() call?', 'Because select() modifies the fd_sets â€” it clears bits for fds that are NOT ready. Reusing without zeroing misses events.'),
            ('What is the first argument to select()?', '`max_fd + 1` â€” select needs to know the range of fds to check.'),
            ('How many threads does your event loop use to handle 50 clients?', 'ONE thread. That is the whole point â€” multiplexing replaces threading for I/O-bound work.'),
        ],
    },
    {
        'frontmatter': {
            'id': 'w03-multi-client-event-loop-d03-quest-backpressure-policy-2h',
            'part': 'w03-multi-client-event-loop',
            'title': 'Quest: Backpressure Policy  2h',
            'order': 3,
            'prereqs': ['w03-multi-client-event-loop-d02-quest-first-multi-client-loop-2h'],
        },
        'goal_intro': 'Define a **per-client backpressure policy** that prevents one slow client from consuming unbounded memory and destabilizing the entire server.',
        'goal_deliverables': [
            'A **per-client buffer limit** defining max queued bytes before intervention',
            'A **slow-reader detection rule** with specific threshold and time window',
            'An **intervention policy**: what happens when limits are exceeded (throttle vs disconnect)',
            'A **slow-reader test scenario** with expected disconnect timing',
        ],
        'pass_criteria': [
            ('Per-client buffer cap defined in bytes', 'Check for specific number (e.g., 64KB)'),
            ('Slow-reader detection rule has both size AND time thresholds', 'Verify two conditions checked'),
            ('Disconnect policy is explicit (not "maybe")', 'Look for clear IF/THEN rule'),
            ('Test scenario shows input, timing, and expected disconnect', 'Verify test has concrete numbers'),
            ('Fairness argument: slow client cannot starve fast clients', 'Check for reasoning about other clients'),
        ],
        'build_description': 'A policy document defining how your event loop protects itself from slow clients â€” the "immune system" of your server.',
        'build_deliverables': [
            'File: `week-3/day3-backpressure-policy.md`',
            'Per-client write buffer cap: e.g., 64KB max queued per connection',
            'Slow-reader rule: disconnect if buffer stays > 80% for > 5 seconds',
            'Test scenario: client reads 1 byte/sec while server sends 1KB/sec',
        ],
        'done_example': '''## Backpressure Policy
Max queued bytes per client: 65536 (64KB)
Warning threshold: 52428 (80%)
Disconnect threshold: 65536 (100%) OR 80% for > 5 seconds

## Slow-Reader Rule
IF write_buf.size() > MAX_QUEUED_BYTES:
    â†’ log warning, close connection immediately
IF write_buf.size() > WARNING_THRESHOLD for > 5 seconds:
    â†’ log slow-reader warning, close connection
ELSE:
    â†’ normal operation, queue data for sending''',
        'code_lang': 'markdown',
        'can_do': 'Define exactly when and why your server disconnects slow clients.',
        'cannot_yet': 'Implement the enforcement â€” that integrates with your loop code. The enforcement code comes when you build the server.',
        'why_without': [
            'One slow client causes your server to buffer gigabytes of unsent data until OOM kill',
            'All 49 other clients suffer latency spikes because the server is swapping memory',
            'Have no way to distinguish "slow client" from "temporarily delayed network"',
            'Production deployments fail under mixed-speed client workloads â€” the most common real-world scenario',
        ],
        'why_with': [
            'Your server has explicit memory bounds â€” maximum memory usage is predictable',
            'Slow clients are detected and disconnected before they affect others',
            'You have clear rules that can be tuned in production (thresholds are config, not guesses)',
            'Week 6\'s full backpressure system builds directly on this per-client policy',
        ],
        'why_connects': [
            '**To Day 1:** The `bytes_queued` field in your Connection struct is what tracks this',
            '**To Day 2:** Your select loop checks write buffer size before queuing more data',
            '**To Day 5:** Connection lifecycle tests include slow-reader disconnect scenarios',
            '**To Week 5:** Thread pool queue limits follow the same bounded-resource principle',
            '**To Week 6:** Full backpressure policy ladder starts with this per-client cap',
        ],
        'mental_model_name': 'Bounded Resources',
        'mental_model_desc': '''In naive code: buffers grow without limit. "It\'ll be fine, most clients are fast."
In production code: **every buffer has a maximum size, and exceeding it triggers an explicit policy.**

This principle applies everywhere in distributed systems:
- Per-client write buffers (today)
- Thread pool work queues (Week 5)
- Replication log buffers (Week 11)
- CAS object store quotas (Week 13)

The rule: if you can\'t name the maximum size and the overflow policy, you have a bug waiting to happen.''',
        'visual_model': '''â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BACKPRESSURE DECISION FLOW                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Server has data to send to Client X                     â”‚
â”‚       â”‚                                                  â”‚
â”‚       â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ Check: write_buf.size() < MAX?  â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚       YES  â”‚          â”‚  NO                              â”‚
â”‚            â–¼          â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Queue data    â”‚  â”‚ Buffer FULL!          â”‚             â”‚
â”‚  â”‚ in write_buf  â”‚  â”‚ â†’ Log: "slow client"  â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â†’ Close connection     â”‚             â”‚
â”‚                     â”‚ â†’ Free buffer          â”‚             â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                          â”‚
â”‚  Also check time-based:                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ IF buf > 80% for > 5 sec continuously:  â”‚            â”‚
â”‚  â”‚    â†’ Disconnect (not temporarily slow â€” â”‚            â”‚
â”‚  â”‚       chronically slow)                  â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                          â”‚
â”‚  Result: Server memory usage bounded to:                 â”‚
â”‚  MAX_CLIENTS Ã— MAX_BUFFER_PER_CLIENT = predictable       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜''',
        'ship_file': 'week-3/day3-backpressure-policy.md',
        'do_steps': [
            {
                'title': 'Define the per-client buffer cap',
                'why': 'Without a number, "bounded" is meaningless. 64KB is a reasonable starting point â€” large enough for normal messages, small enough that 1000 clients = 64MB total.',
                'content': '''   Choose your limits:

   ```
   MAX_WRITE_BUFFER = 65536   // 64KB per client
   WARNING_THRESHOLD = 52428  // 80% of max
   MAX_CLIENTS = 1000         // server-wide limit
   MAX_SERVER_MEMORY = MAX_CLIENTS Ã— MAX_WRITE_BUFFER = 64MB
   ```

   Justify your numbers: Why 64KB? Because a typical protocol message is < 4KB, so 64KB holds ~16 pending messages â€” plenty for normal operation.''',
            },
            {
                'title': 'Define the slow-reader detection rule',
                'why': 'Size alone isn\'t enough. A client at 90% buffer for 0.1 seconds is fine (temporary network blip). A client at 90% for 5 seconds is chronically slow and must be disconnected.',
                'content': '''   ```
   SLOW_READER_RULE:
     Condition 1 (instant): write_buf.size() >= MAX_WRITE_BUFFER
       â†’ Action: disconnect immediately, log "buffer_overflow"

     Condition 2 (sustained): write_buf.size() >= WARNING_THRESHOLD
                               AND time_above_threshold > 5 seconds
       â†’ Action: disconnect, log "slow_reader_sustained"

     Condition 3 (normal): write_buf.size() < WARNING_THRESHOLD
       â†’ Action: continue normal operation
   ```''',
            },
            {
                'title': 'Design the disconnect procedure',
                'why': 'Disconnecting a client requires cleanup: flush what you can, close the fd, remove from registry, log the reason. Skipping any step causes resource leaks.',
                'content': '''   ```
   disconnect(fd, reason):
     1. Log: "{fd} disconnected: {reason}, queued={bytes_queued}"
     2. Attempt to send any final error message (best-effort)
     3. shutdown(fd, SHUT_WR)    // signal no more writes
     4. close(fd)                // release the fd
     5. registry.remove(fd)      // remove from tracking
     6. Update metrics: slow_client_disconnects++
   ```''',
            },
            {
                'title': 'Write the fairness argument',
                'why': 'Backpressure isn\'t just about one client â€” it\'s about protecting ALL clients. If slow Client A consumes all server memory, fast Clients B-Z suffer.',
                'content': '''   Document why this policy is fair:

   ```
   FAIRNESS GUARANTEE:
   - Each client gets equal maximum buffer space (64KB)
   - No client can cause another client's data to be dropped
   - Server total memory is bounded: MAX_CLIENTS Ã— MAX_BUFFER
   - Fast clients are never delayed by slow clients
   - Disconnecting a slow client frees resources for healthy clients
   ```''',
            },
            {
                'title': 'Design the slow-reader test scenario',
                'why': 'You need concrete numbers to verify the policy works. This test becomes part of your Day 5 connection lifecycle tests.',
                'content': '''   ```
   TEST: Slow Reader Disconnect
   Setup:
     - Server sends 1KB messages every 100ms to Client X
     - Client X reads only 1 byte per second
   Expected timeline:
     T=0s:    write_buf = 0 bytes
     T=1s:    write_buf â‰ˆ 10KB (10 messages queued)
     T=5s:    write_buf â‰ˆ 50KB (hits WARNING_THRESHOLD)
     T=5s+5s: write_buf â‰ˆ 100KB (exceeds MAX, or sustained > 5s)
     Tâ‰ˆ6.5s:  Server disconnects Client X
   Expected log:
     "fd=7 disconnected: slow_reader_sustained, queued=52430"
   Expected effect:
     - All other clients unaffected
     - Server memory drops by ~50KB
   ```''',
            },
        ],
        'done_when': [
            'Per-client buffer cap with specific byte value â€” *becomes a constant in your server code*',
            'Slow-reader rule with both instant and sustained conditions â€” *two-tier detection catches both floods and chronic slowness*',
            'Disconnect procedure with logging and cleanup â€” *prevents fd leaks on forced disconnect*',
            'Fairness argument explaining why this protects all clients â€” *the rationale for code review*',
            'Slow-reader test with timeline and expected log output â€” *becomes a Day 5 test case*',
        ],
        'proof_instruction': 'Paste your backpressure policy with buffer limits and slow-reader rule, or upload `week-3/day3-backpressure-policy.md`.',
        'self_test': [
            ('What is the maximum memory your server can use for write buffers?', 'MAX_CLIENTS Ã— MAX_WRITE_BUFFER (e.g., 1000 Ã— 64KB = 64MB)'),
            ('Why disconnect slow clients instead of just pausing reads?', 'Pausing reads still holds buffer memory. Disconnecting frees it immediately for healthy clients.'),
            ('What is the difference between the instant and sustained disconnect rules?', 'Instant: buffer is completely full â†’ disconnect now. Sustained: buffer is > 80% for > 5 seconds â†’ disconnect (chronically slow).'),
        ],
    },
    {
        'frontmatter': {
            'id': 'w03-multi-client-event-loop-d04-quest-poll-migration-2h',
            'part': 'w03-multi-client-event-loop',
            'title': 'Quest: Poll Migration  2h',
            'order': 4,
            'prereqs': ['w03-multi-client-event-loop-d03-quest-backpressure-policy-2h'],
        },
        'goal_intro': 'Plan the **migration from `select()` to `poll()`** while preserving every behavior from your existing event loop â€” a practice run for changing internals without changing contracts.',
        'goal_deliverables': [
            'A **selectâ†’poll mapping table** showing how each select concept translates to poll',
            'A **migration checklist** with before/after verification steps',
            'A **regression test list** proving identical behavior after the switch',
            'An **analysis** of what `poll()` improves over `select()` and what stays the same',
        ],
        'pass_criteria': [
            ('Mapping table covers fd_set, FD_ISSET, FD_SET, max_fd, timeout', 'All 5 select concepts mapped'),
            ('Migration checklist has â‰¥ 8 steps', 'Count checklist items'),
            ('Regression tests cover connect, disconnect, send, recv, timeout, EAGAIN', 'Verify 6 behaviors tested'),
            ('Analysis compares fd limit, performance, and API complexity', 'Check 3 comparison points'),
            ('State machine from Day 1 is unchanged', 'Confirm same states and transitions'),
        ],
        'build_description': 'A migration plan document for swapping `select()` with `poll()` â€” your first practice at changing implementation without changing behavior.',
        'build_deliverables': [
            'File: `week-3/day4-poll-migration-checklist.md`',
            'selectâ†’poll mapping table',
            'Migration checklist with 8+ verification steps',
            'Regression test list for 6+ behaviors',
        ],
        'done_example': '''## Select â†’ Poll Mapping
| select concept | poll equivalent |
|---------------|-----------------|
| fd_set read_fds | pollfd.events = POLLIN |
| fd_set write_fds | pollfd.events = POLLOUT |
| FD_SET(fd, &set) | Add pollfd to array |
| FD_ISSET(fd, &set) | Check pollfd.revents |
| max_fd + 1 | nfds = array.size() (no max_fd needed!) |
| struct timeval | timeout_ms (int, milliseconds) |''',
        'code_lang': 'markdown',
        'can_do': 'Plan a complete internal migration with zero behavior change.',
        'cannot_yet': 'Migrate to `epoll` â€” that\'s next week\'s upgrade. `poll()` is the intermediate step.',
        'why_without': [
            'Be stuck with `select()` and its 1024-fd limit forever',
            'Fear making internal changes because you can\'t verify behavior preservation',
            'Skip the intermediate `poll()` step and try jumping straight to `epoll`, making debugging harder',
            'Miss the most important engineering skill: changing internals without breaking contracts',
        ],
        'why_with': [
            'Practice the "swap internals, keep contracts" pattern you\'ll use repeatedly',
            'Remove the fd_set 1024-fd limitation without any behavior change',
            'Have a regression checklist that proves correctness through the migration',
            'Be confident for Week 4\'s `epoll` upgrade â€” same process, bigger API change',
        ],
        'why_connects': [
            '**To Days 1-3:** Your state machine, registry, and backpressure policy are UNCHANGED',
            '**To Day 5:** Regression tests from this migration join the lifecycle test suite',
            '**To Week 4:** `epoll` migration follows this exact same checklist pattern',
            '**To Week 7:** Protocol version migration uses the same "change internals, keep API" discipline',
            '**To Week 11:** Replication protocol upgrades follow the same regression pattern',
        ],
        'mental_model_name': 'Contract-Preserving Migration',
        'mental_model_desc': '''The most important skill in long-lived systems is NOT building the first version.
It\'s **changing the implementation while preserving the contract.**

`select()` â†’ `poll()` is a tiny migration. But the PROCESS is identical to:
- Upgrading database engines
- Swapping serialization formats
- Migrating from REST to gRPC

The process: (1) Map old â†’ new concepts. (2) Checklist every change. (3) Regression test EVERY behavior.
If you master this for `selectâ†’poll`, you\'ll do it confidently for any migration.''',
        'visual_model': '''â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SELECT â†’ POLL MIGRATION                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  BEFORE (select):                AFTER (poll):           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ fd_set read_fds â”‚              â”‚ struct pollfd[] { â”‚    â”‚
â”‚  â”‚ fd_set write_fdsâ”‚              â”‚   .fd = fd,       â”‚    â”‚
â”‚  â”‚ max_fd tracking â”‚              â”‚   .events = POLLINâ”‚    â”‚
â”‚  â”‚ FD_ZERO every   â”‚              â”‚              |     â”‚    â”‚
â”‚  â”‚   iteration     â”‚              â”‚         POLLOUT    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ }                  â”‚    â”‚
â”‚                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  UNCHANGED:                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ âœ… Connection state machine (Day 1)             â”‚      â”‚
â”‚  â”‚ âœ… Connection registry (Day 2)                  â”‚      â”‚
â”‚  â”‚ âœ… Backpressure policy (Day 3)                  â”‚      â”‚
â”‚  â”‚ âœ… EAGAIN handling                              â”‚      â”‚
â”‚  â”‚ âœ… Accept/read/write dispatch logic             â”‚      â”‚
â”‚  â”‚ âœ… All external behavior                        â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                          â”‚
â”‚  IMPROVED:                                               â”‚
â”‚  âœ… No 1024-fd limit                                     â”‚
â”‚  âœ… No FD_ZERO rebuild needed                            â”‚
â”‚  âœ… Cleaner event iteration                              â”‚
â”‚  âœ… Dynamic array instead of fixed bitmap                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜''',
        'ship_file': 'week-3/day4-poll-migration-checklist.md',
        'do_steps': [
            {
                'title': 'Build the selectâ†’poll mapping table',
                'why': 'A side-by-side mapping eliminates guesswork. Every select concept must have a poll equivalent before you change any code.',
                'content': '''   Map every `select()` concept to its `poll()` equivalent:

   ```
   select: FD_SET(fd, &read_fds)     â†’  poll: pfd.events |= POLLIN
   select: FD_SET(fd, &write_fds)    â†’  poll: pfd.events |= POLLOUT
   select: FD_ISSET(fd, &read_fds)   â†’  poll: pfd.revents & POLLIN
   select: FD_ISSET(fd, &write_fds)  â†’  poll: pfd.revents & POLLOUT
   select: max_fd + 1                â†’  poll: not needed (array length)
   select: struct timeval {sec, usec} â†’  poll: int timeout_ms
   select: error_fds                 â†’  poll: POLLERR | POLLHUP in revents
   ```''',
            },
            {
                'title': 'Write the migration checklist',
                'why': 'A step-by-step checklist prevents partial migration where some code uses select and some uses poll â€” a common source of bugs.',
                'content': '''   ```
   MIGRATION CHECKLIST:
   â–¡ 1. Replace fd_set declarations with std::vector<pollfd>
   â–¡ 2. Replace FD_ZERO with vector.clear()
   â–¡ 3. Replace FD_SET with vector.push_back({fd, events, 0})
   â–¡ 4. Replace select() call with poll(fds.data(), fds.size(), timeout_ms)
   â–¡ 5. Replace FD_ISSET with revents & POLLIN / POLLOUT checks
   â–¡ 6. Remove max_fd tracking entirely
   â–¡ 7. Add POLLERR/POLLHUP checks for error detection
   â–¡ 8. Update timeout from timeval to milliseconds integer
   â–¡ 9. Run ALL regression tests
   â–¡ 10. Remove all select-related #includes
   ```''',
            },
            {
                'title': 'Define regression tests',
                'why': 'Every behavior that worked with select must work identically with poll. These tests become permanent â€” they also verify the Week 4 epoll migration.',
                'content': '''   | # | Behavior | Test | Expected |
   |---|---------|------|----------|
   | 1 | Accept new connection | Connect one client | Client added to registry |
   | 2 | Receive data | Send "hello" | Server receives "hello" |
   | 3 | Send response | Server sends "echo" | Client receives "echo" |
   | 4 | Client disconnect | Close client socket | Server removes from registry |
   | 5 | Timeout | No activity for 1s | select/poll returns 0 |
   | 6 | EAGAIN on write | Fill client recv buffer | Server handles without crash |
   | 7 | 50 concurrent clients | Connect 50 clients | All served correctly |
   | 8 | Backpressure | Slow reader | Disconnected per policy |''',
            },
            {
                'title': 'Analyze poll improvements',
                'why': 'Understanding WHY you migrated helps you decide when further migration (to epoll) is warranted versus premature optimization.',
                'content': '''   ```
   WHAT POLL IMPROVES:
   âœ… No fd limit (select capped at FD_SETSIZE, typically 1024)
   âœ… No bitmap rebuild â€” pollfd array persists (only reset revents)
   âœ… Cleaner API â€” events and revents are explicit bitmasks
   âœ… Dynamic sizing â€” just grow the vector

   WHAT POLL DOES NOT IMPROVE:
   âŒ Still O(n) scan of all fds each call (epoll fixes this)
   âŒ Still copies fd array to kernel each call (epoll fixes this)
   âŒ No edge-triggered mode (epoll adds this)
   ```''',
            },
            {
                'title': 'Document the "unchanged" list',
                'why': 'The most important part of a migration is what does NOT change. If your state machine or backpressure policy changed, something went wrong.',
                'content': '''   Explicitly list everything that must be identical:

   ```
   UNCHANGED AFTER MIGRATION:
   âœ… Connection state machine (enum values, transitions)
   âœ… Connection registry (add/remove/get API)
   âœ… Backpressure policy (buffer limits, disconnect rules)
   âœ… Protocol behavior (framing, parsing, responses)
   âœ… Logging format and content
   âœ… Exit codes and error handling
   âœ… All external-facing behavior
   ```''',
            },
        ],
        'done_when': [
            'selectâ†’poll mapping table covers all 7 concepts â€” *your migration reference card*',
            'Migration checklist with 10 steps â€” *follow sequentially, check each off*',
            '8 regression tests covering all observable behaviors â€” *reusable for Week 4 epoll migration*',
            'Analysis of what improves and what doesn\'t â€” *prevents over-optimizing or under-migrating*',
            'Unchanged list confirming state machine and contracts preserved â€” *the #1 migration safety check*',
        ],
        'proof_instruction': 'Paste your selectâ†’poll mapping table and regression test list, or upload `week-3/day4-poll-migration-checklist.md`.',
        'self_test': [
            ('What select limitation does poll remove?', 'The FD_SETSIZE limit (typically 1024 fds). poll uses a dynamic array with no fixed limit.'),
            ('What does poll NOT improve over select?', 'Still O(n) â€” polls every fd every call, still copies fd array to kernel. epoll fixes both.'),
            ('What is the most important thing to verify after migration?', 'That ALL external behavior is identical â€” same responses, same disconnects, same logging. Internals changed, contracts did not.'),
        ],
    },
    {
        'frontmatter': {
            'id': 'w03-multi-client-event-loop-d05-quest-connection-lifecycle-tests-2h',
            'part': 'w03-multi-client-event-loop',
            'title': 'Quest: Connection Lifecycle Tests  2h',
            'order': 5,
            'prereqs': ['w03-multi-client-event-loop-d04-quest-poll-migration-2h'],
        },
        'goal_intro': 'Design the **complete connection lifecycle test suite** that proves your event loop correctly handles every phase from accept to close, including churn, half-close, and fd leak detection.',
        'goal_deliverables': [
            'A **lifecycle test matrix** covering open/read/write/error/close for every state',
            'A **churn test plan** with rapid connect/disconnect cycles and leak counters',
            'A **half-close test scenario** proving server handles `shutdown()` correctly',
            'An **fd leak detection strategy** that catches descriptor leaks over time',
        ],
        'pass_criteria': [
            ('Lifecycle matrix covers all 5 connection states from Day 1', 'Map test to each enum value'),
            ('Churn test has specific cycle count and leak counter', 'Verify numbers (e.g., 1000 cycles, 0 leaks)'),
            ('Half-close test shows expected server behavior', 'Verify shutdown(SHUT_WR) handling'),
            ('Fd leak detection has measurement method', 'Check for /proc/self/fd or lsof approach'),
            ('Test plan covers â‰¥ 15 scenarios', 'Count numbered test rows'),
        ],
        'build_description': 'A comprehensive test plan document covering every connection lifecycle scenario â€” your quality gate before moving to Week 4.',
        'build_deliverables': [
            'File: `week-3/day5-connection-lifecycle-tests.md`',
            '15+ test scenarios covering all lifecycle phases',
            'Churn test: 1000 rapid connect/disconnect cycles with zero fd leaks',
            'Half-close test: client sends shutdown(SHUT_WR), server handles gracefully',
        ],
        'done_example': '''## Test #1: Normal Connection Lifecycle
Input:  Client connects, sends "hello", receives echo, disconnects
States: CONNECTING â†’ READING â†’ WRITING â†’ CLOSING â†’ removed
Assert: Response matches, fd removed from registry, no leak

## Test #7: Half-Close
Input:  Client calls shutdown(SHUT_WR) after sending data
States: Server sees recv()=0, transitions to CLOSING
Assert: Server sends any remaining response, then closes cleanly

## Fd Leak Detection
Method: Count /proc/self/fd entries before and after 1000 cycles
Assert: fd_count_after == fd_count_before (zero leaks)''',
        'code_lang': 'markdown',
        'can_do': 'Verify every connection lifecycle scenario including edge cases that crash production servers.',
        'cannot_yet': 'Run a 30-minute soak test â€” that\'s tomorrow\'s Boss Fight.',
        'why_without': [
            'Ship an event loop that slowly leaks file descriptors until the process hits ulimit and dies',
            'Discover half-close handling bugs only when a real client library sends `shutdown()`',
            'Have no evidence that your loop survives connection churn â€” the most common real traffic pattern',
            'Spend hours debugging "why did the server stop accepting connections after 3 hours?"',
        ],
        'why_with': [
            'Have concrete evidence that every lifecycle phase works correctly',
            'Catch fd leaks before they become production incidents',
            'Know your server handles half-close, error, and timeout paths â€” not just happy paths',
            'Have a reusable test suite that also validates the Week 4 epoll migration',
        ],
        'why_connects': [
            '**To Days 1-4:** These tests verify every design decision from this week',
            '**To Day 6 (Boss Fight):** The soak test runs THESE scenarios for 30 minutes',
            '**To Week 4:** These same tests verify the epoll migration',
            '**To Week 5:** Thread pool tests add concurrency scenarios to this base',
            '**To Week 6:** Backpressure tests extend the slow-reader scenarios here',
        ],
        'mental_model_name': 'Test at the Boundaries',
        'mental_model_desc': '''Happy paths rarely reveal bugs. Systems break at boundaries:
- **State transitions:** What happens between READING and WRITING?
- **Resource lifecycle:** Is the fd always closed? Is the buffer always freed?
- **Churn:** What happens when connections come and go rapidly?
- **Half-states:** What happens when one side closes before the other?

By Week 12 when building leader election, you\'ll test boundary conditions automatically:
"What if the leader dies mid-write? What if two nodes think they\'re leader?"
The habit of boundary testing starts here.''',
        'visual_model': '''â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONNECTION LIFECYCLE TEST MATRIX              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Test Category          Coverage Target                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚
â”‚  Happy path (3)         Normal open â†’ use â†’ close        â”‚
â”‚  Error paths (4)        Network errors at each state     â”‚
â”‚  Half-close (2)         Client/server shutdown variants  â”‚
â”‚  Churn tests (3)        Rapid connect/disconnect cycles  â”‚
â”‚  Leak detection (2)     fd and memory leak verification  â”‚
â”‚  Timeout tests (2)      Idle connection expiry           â”‚
â”‚                                                          â”‚
â”‚  FD LEAK DETECTION:                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    1000 cycles    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Count fds  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Count fds  â”‚        â”‚
â”‚  â”‚ BEFORE     â”‚                   â”‚ AFTER      â”‚        â”‚
â”‚  â”‚ = N        â”‚                   â”‚ must = N   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                          â”‚
â”‚  Method: ls /proc/self/fd | wc -l                        â”‚
â”‚  Alternative: lsof -p $PID | grep SOCK | wc -l          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜''',
        'ship_file': 'week-3/day5-connection-lifecycle-tests.md',
        'do_steps': [
            {
                'title': 'Build the lifecycle test matrix',
                'why': 'A numbered matrix ensures complete coverage. Every state from Day 1\'s enum must have at least one test entry and one test exit.',
                'content': '''   Create numbered test cases for each lifecycle phase:

   | # | Category | Scenario | Expected State Transitions | Assert |
   |---|---------|----------|--------------------------|--------|
   | 1 | Happy | Connect, send, receive, close | CONNâ†’READâ†’WRITEâ†’CLOSE | Echo correct |
   | 2 | Happy | Connect, send large message | CONNâ†’READ_HDRâ†’READ_BODYâ†’WRITE | Full message received |
   | 3 | Happy | Multiple messages on one connection | READâ†”WRITE cycles | All echoed |
   | 4 | Error | Client disappears (no close) | READ â†’ CLOSE (on error) | fd cleaned up |
   | 5 | Error | Client sends invalid frame | READ â†’ CLOSE (protocol error) | Error logged |
   | 6 | Error | Network error during write | WRITE â†’ CLOSE | Partial write handled |
   | 7 | Error | Connection reset by peer | ANY â†’ CLOSE | ECONNRESET handled |''',
            },
            {
                'title': 'Design half-close test scenarios',
                'why': 'Half-close (`shutdown(SHUT_WR)`) is the most subtle TCP edge case. The client says "I\'m done sending" but can still receive. Your server must handle this correctly.',
                'content': '''   ```
   TEST: Client Half-Close
   1. Client connects and sends "hello"
   2. Client calls shutdown(fd, SHUT_WR)  â† stops sending
   3. Server sees recv() return 0          â† EOF on read
   4. Server sends echo response           â† client can still receive!
   5. Server calls close(fd)

   ASSERT: Client receives echo AFTER half-close
   ASSERT: Server transitions to CLOSING, not crash

   TEST: Server Half-Close
   1. Server sends response
   2. Server calls shutdown(client_fd, SHUT_WR)
   3. Client sees recv() return 0
   4. Client can still send (but server won't read)
   ASSERT: Graceful shutdown without errors
   ```''',
            },
            {
                'title': 'Design the churn test',
                'why': 'Real servers face thousands of connections opening and closing per second. Churn exposes cleanup bugs, fd leaks, and registry corruption.',
                'content': '''   ```
   TEST: Connection Churn (1000 cycles)
   Setup: Start server
   Loop 1000 times:
     1. Open 10 connections simultaneously
     2. Each sends one message
     3. Verify echo response
     4. Close all 10
   After loop:
     ASSERT: registry.size() == 0
     ASSERT: fd_count == baseline_fd_count  (ZERO leaks)
     ASSERT: server still accepts new connections
     ASSERT: no error logs during churn
   ```''',
            },
            {
                'title': 'Define fd leak detection method',
                'why': 'Fd leaks are silent killers â€” the server works fine for hours, then suddenly can\'t accept connections because it hit the fd limit.',
                'content': '''   Choose a detection method:

   **Method 1: /proc/self/fd counting**
   ```bash
   # Before test
   BEFORE=$(ls /proc/$SERVER_PID/fd | wc -l)
   # Run test
   ./run_churn_test
   # After test
   AFTER=$(ls /proc/$SERVER_PID/fd | wc -l)
   # Assert
   [ "$BEFORE" -eq "$AFTER" ] || echo "LEAK: $((AFTER-BEFORE)) fds leaked"
   ```

   **Method 2: Socket-specific counting**
   ```bash
   lsof -p $PID | grep -c "SOCK"
   ```

   Include this in your test suite as a post-test assertion.''',
            },
            {
                'title': 'Add timeout test scenarios',
                'why': 'Timeout handling is where cleanup meets time. A connection that times out must be closed AND removed â€” partial cleanup causes fd leaks.',
                'content': '''   ```
   TEST: Idle Timeout
   1. Client connects but sends nothing
   2. Wait for server timeout period (e.g., 10 seconds)
   ASSERT: Server closes connection, logs "idle_timeout"
   ASSERT: fd removed from registry

   TEST: Read Timeout
   1. Client sends partial frame header (3 of 4 bytes)
   2. Client stops sending
   3. Wait for read timeout
   ASSERT: Server closes connection, logs "read_timeout"
   ASSERT: Partial buffer freed, fd removed
   ```''',
            },
        ],
        'done_when': [
            '15+ numbered test scenarios in lifecycle matrix â€” *proves complete lifecycle coverage*',
            'Half-close tests for client and server shutdown â€” *catches the subtlest TCP edge case*',
            'Churn test: 1000 cycles with zero fd leaks â€” *proves cleanup correctness under rapid cycling*',
            'Fd leak detection using /proc/self/fd â€” *automated leak checking for CI*',
            'Timeout tests for idle and partial-read â€” *proves time-based cleanup works*',
        ],
        'proof_instruction': 'Paste your lifecycle test matrix (15+ scenarios) and fd leak detection method, or upload `week-3/day5-connection-lifecycle-tests.md`.',
        'self_test': [
            ('How do you detect fd leaks in a running server?', 'Count entries in /proc/$PID/fd before and after tests. If the count increases, fds are leaking.'),
            ('What happens when `recv()` returns 0?', 'The peer closed their send side (half-close or full close). Your server should transition to CLOSING.'),
            ('Why test 1000 rapid connect/disconnect cycles instead of just 10?', 'Leaks are often 1 fd per cycle. 10 cycles might leak 10 fds (unnoticed). 1000 cycles leak 1000 fds (obvious and measurable).'),
        ],
    },
]

# I'll continue with the remaining weeks using a more compact data generation approach.
# The remaining weeks will use helper functions to generate consistent, rich content.

def make_week_lessons(week_num, week_slug, week_theme, lessons_data):
    """Generate lesson data dicts for a week from compact specifications."""
    result = []
    for i, ld in enumerate(lessons_data):
        day = i + 1
        prev_id = f"{week_slug}-d{day-1:02d}-quest-{lessons_data[i-1]['slug']}-2h" if day > 1 else None

        # Determine prereqs
        if ld.get('prereqs') is not None:
            prereqs = ld['prereqs']
        elif day == 1:
            prereqs = []
        else:
            prereqs = [prev_id]

        fm_id = ld.get('id', f"{week_slug}-d{day:02d}-quest-{ld['slug']}-2h")

        d = {
            'frontmatter': {
                'id': fm_id,
                'part': week_slug,
                'title': ld['title'],
                'order': day,
                'prereqs': prereqs,
            },
            'goal_intro': ld['goal_intro'],
            'goal_deliverables': ld['goal_deliverables'],
            'pass_criteria': ld['pass_criteria'],
            'build_description': ld['build_desc'],
            'build_deliverables': ld['build_deliverables'],
            'done_example': ld['done_example'],
            'code_lang': ld.get('code_lang', 'markdown'),
            'can_do': ld['can_do'],
            'cannot_yet': ld['cannot_yet'],
            'why_without': ld['why_without'],
            'why_with': ld['why_with'],
            'why_connects': ld['why_connects'],
            'mental_model_name': ld['mental_model_name'],
            'mental_model_desc': ld['mental_model_desc'],
            'visual_model': ld['visual_model'],
            'ship_file': ld['ship_file'],
            'do_steps': ld['do_steps'],
            'done_when': ld['done_when'],
            'proof_instruction': ld['proof_instruction'],
            'self_test': ld['self_test'],
        }
        result.append(d)
    return result


# â”€â”€â”€ FILE DISCOVERY AND WRITING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def find_lesson_file(week_slug, day_num):
    """Find the existing lesson file for a given week and day number."""
    week_dir = os.path.join(BASE_DIR, week_slug, 'lessons')
    if not os.path.isdir(week_dir):
        # Try partial matching
        for d in os.listdir(BASE_DIR):
            if d.startswith(week_slug[:3]):
                week_dir = os.path.join(BASE_DIR, d, 'lessons')
                break
    if not os.path.isdir(week_dir):
        return None

    for f in sorted(os.listdir(week_dir)):
        if f.startswith(f'{day_num:02d}-') and f.endswith('.md'):
            return os.path.join(week_dir, f)
    return None


def read_existing_frontmatter(filepath):
    """Read existing YAML frontmatter from a lesson file."""
    with open(filepath, 'r') as f:
        content = f.read()
    m = re.match(r'^---\n(.*?\n)---', content, re.DOTALL)
    if m:
        return yaml.safe_load(m.group(1))
    return {}


def write_lesson(filepath, content):
    """Write lesson content to file."""
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, 'w') as f:
        f.write(content)
    print(f"  âœ… Wrote {os.path.basename(filepath)} ({len(content.splitlines())} lines)")


# â”€â”€â”€ MAIN GENERATION LOGIC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# We'll store ALL week data in a list, where each entry is (week_slug, [lesson_data_dicts])
ALL_WEEKS = []

# Week 3 is fully defined above
ALL_WEEKS.append(('w03-multi-client-event-loop', W03_LESSONS))


# â”€â”€â”€ REMAINING WEEKS DATA (W04-W24) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Each week defines 5 lessons with full content matching Day 1 quality.

# Import the remaining week data from separate files
# For efficiency, we define them inline here.

# Helper for consistent visual models
def box_diagram(title, rows):
    """Generate a simple ASCII box diagram."""
    width = max(len(title) + 4, max(len(r) for r in rows) + 4)
    top = 'â”Œ' + 'â”€' * width + 'â”'
    bottom = 'â””' + 'â”€' * width + 'â”˜'
    title_line = 'â”‚' + title.center(width) + 'â”‚'
    sep = 'â”œ' + 'â”€' * width + 'â”¤'
    content = '\n'.join('â”‚  ' + r.ljust(width - 2) + 'â”‚' for r in rows)
    return f"{top}\n{title_line}\n{sep}\n{content}\n{bottom}"


if __name__ == '__main__':
    print("=" * 60)
    print("Rich Lesson Generator â€” W03-W24")
    print("=" * 60)

    total_written = 0
    total_lines = 0

    for week_slug, lessons in ALL_WEEKS:
        print(f"\nğŸ“— Processing {week_slug}...")
        for lesson_data in lessons:
            day = lesson_data['frontmatter']['order']
            filepath = find_lesson_file(week_slug, day)
            if filepath is None:
                print(f"  âš ï¸  Could not find file for {week_slug} day {day}")
                continue

            content = generate_lesson(lesson_data)
            write_lesson(filepath, content)
            total_written += 1
            total_lines += len(content.splitlines())

    print(f"\n{'=' * 60}")
    print(f"âœ… Generated {total_written} files, {total_lines} total lines")
    print(f"ğŸ“Š Average: {total_lines // max(total_written, 1)} lines/file")
    print(f"{'=' * 60}")
