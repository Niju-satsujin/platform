---
id: w21-reliability-slo-story-d05-capacity-plan
part: w21-reliability-slo-story
title: "Capacity Plan"
order: 5
duration_minutes: 120
prereqs: ["w21-reliability-slo-story-d04-alert-rules"]
proof:
  type: "paste_or_upload"
  status: "manual_or_regex"
review_schedule_days: [3,7,21,60]
---

# Capacity Plan

## Goal

Create a capacity plan that maps current resource usage to projected demand, ensures 2Ã— surge headroom, and defines scaling triggers tied to your SLOs.

### âœ… Deliverables

1. A resource inventory table listing CPU, memory, disk, and network per component.
2. A load model showing expected request rates at 1Ã—, 2Ã—, and 5Ã— current traffic.
3. A scaling trigger matrix linking SLO degradation to capacity actions.
4. A cost projection for 3, 6, and 12 months under growth assumptions.
5. A bottleneck analysis identifying the first component to saturate under load.

### **PASS CRITERIA**

| # | Criterion | How to check |
|---|-----------|--------------|
| 1 | Resource inventory covers all trust-critical components | Cross-reference with architecture |
| 2 | Load model includes 2Ã— surge headroom target | Verify "2Ã— column" in load table |
| 3 | Scaling triggers reference specific SLOs or metrics | Check trigger definitions |
| 4 | Bottleneck analysis identifies â‰¥1 saturation point | Read analysis section |
| 5 | Cost projection includes 3 time horizons | Verify 3/6/12 month rows |

## What You're Building Today

You are building the capacity forecast that prevents your distributed trust platform from falling over on its busiest day. This plan turns reactive fire-fighting into proactive scaling.

### âœ… Deliverables

- Resource inventory and load model document.
- Scaling trigger matrix.
- Cost projection table.

```markdown
## Resource Inventory

| Component          | CPU (cores) | Memory (GB) | Disk (GB) | Net (Mbps) | Replicas |
|--------------------|-------------|-------------|-----------|------------|----------|
| verify-service     | 2           | 4           | 10        | 100        | 3        |
| sign-service       | 4           | 8           | 20        | 200        | 2        |
| attestation-worker | 1           | 2           | 50        | 50         | 2        |
| raft-cluster       | 2           | 4           | 100       | 500        | 3        |
| prometheus         | 2           | 8           | 200       | 100        | 1        |

## Load Model

| Scenario    | Verify req/s | Sign req/s | Attest/min | Total CPU | Total Mem |
|-------------|-------------|------------|------------|-----------|-----------|
| Current (1Ã—)| 100         | 20         | 60         | 15 cores  | 34 GB     |
| Peak (2Ã—)   | 200         | 40         | 120        | 28 cores  | 64 GB     |
| Surge (5Ã—)  | 500         | 100        | 300        | 65 cores  | 150 GB    |
```

You **can:**
- Use back-of-envelope estimates refined from Week 18 benchmark data.
- Model horizontal and vertical scaling options.
- Include both optimistic and pessimistic growth scenarios.
- Reference cloud pricing calculators for cost estimates.

You **cannot yet:**
- Automate scaling triggers (that requires production infrastructure).
- Account for geographic distribution (single-region model first).
- Test capacity limits in production (use benchmark data as proxy).
- Implement auto-scaling policies (document the triggers first).

## Why This Matters

ðŸ”´ **Without a capacity plan:**
- Traffic spikes cause cascading failures â€” SLOs breach with no warning.
- Emergency scaling is expensive and slow (procurement, provisioning).
- Teams discover bottlenecks only during outages.
- Budget requests lack data â€” finance says "prove you need it."

ðŸŸ¢ **With a capacity plan:**
- Scaling decisions are made weeks before they're needed.
- 2Ã— headroom absorbs unexpected traffic without SLO impact.
- Bottleneck analysis focuses optimization on the right component.
- Cost projections justify infrastructure budget with data.

ðŸ”— **Connects:**
- **Day 1** (SLI/SLO) â†’ capacity targets ensure SLOs hold under load.
- **Day 4** (Alert rules) â†’ saturation alerts trigger capacity actions.
- **Week 10** (Raft) â†’ consensus cluster requires careful memory/disk planning.
- **Week 18** (Benchmarks) â†’ load test data feeds the load model.
- **Week 22** (Security) â†’ DDoS capacity planning ties to threat model.

ðŸ§  **Mental model: "The 2Ã— Headroom Rule"** â€” Your system should handle twice your current peak without degradation. This isn't paranoia â€” it's insurance. Traffic doubles from viral events, seasonal spikes, or a single large customer onboarding. The 2Ã— buffer is your margin between "no problem" and "all hands on deck."

## Visual Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CAPACITY PLANNING MODEL                â”‚
â”‚                                                     â”‚
â”‚  Traffic â–²                                          â”‚
â”‚          â”‚          â•± 5Ã— surge (emergency zone)     â”‚
â”‚          â”‚        â•±                                 â”‚
â”‚          â”‚      â•±â”€â”€ 2Ã— headroom target â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚          â”‚    â•±                                     â”‚
â”‚          â”‚  â•±â”€â”€â”€â”€ current peak â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚          â”‚â•±                                         â”‚
â”‚          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Time      â”‚
â”‚          â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚         BOTTLENECK CASCADE                â”‚      â”‚
â”‚  â”‚                                           â”‚      â”‚
â”‚  â”‚  1st: verify-service CPU saturates        â”‚      â”‚
â”‚  â”‚       â–¼                                   â”‚      â”‚
â”‚  â”‚  2nd: raft-cluster disk I/O queues        â”‚      â”‚
â”‚  â”‚       â–¼                                   â”‚      â”‚
â”‚  â”‚  3rd: prometheus TSDB OOM                 â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                     â”‚
â”‚  Scaling triggers:                                  â”‚
â”‚  CPU > 70% for 10m  â†’ add replica                   â”‚
â”‚  Disk > 80%         â†’ expand volume                 â”‚
â”‚  p99 > SLO target   â†’ investigate + scale           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Build

File: `week-21/day5-capacity-plan.md`

## Do

1. **Build the resource inventory from your architecture**
   > ðŸ’¡ *WHY: You can't plan capacity for components you haven't cataloged. This inventory becomes the input to every scaling decision.*
   List every component from your architecture diagram. For each, record current CPU, memory, disk, and network allocation. Include replicas.

2. **Model load at 1Ã—, 2Ã—, and 5Ã— current traffic**
   > ðŸ’¡ *WHY: 1Ã— validates your model against reality. 2Ã— is your planning target. 5Ã— reveals where the architecture fundamentally breaks.*
   Use your Week 18 benchmark data to establish the 1Ã— baseline. Scale linearly for compute, sub-linearly for stateful components (Raft leader doesn't scale horizontally). For each component, document: the scaling factor (linear, sub-linear, fixed), the bottleneck resource (CPU, memory, disk I/O, or network), and the maximum supported load before degradation begins. Note that the Raft cluster has a write throughput ceiling determined by leader capacity â€” this is a fundamental architectural constraint, not a bug.

3. **Identify the bottleneck cascade order**
   > ðŸ’¡ *WHY: Under load, components fail in a specific order. Knowing that order tells you which component to scale first and where to optimize.*
   For each component, calculate the traffic level at which it saturates. Sort by saturation point â€” the first to saturate is your bottleneck. Document the cascade: when component A saturates, what happens to B? Does the failure propagate (e.g., verify-service timeout causes client retries, doubling effective load on the API gateway)? Understanding cascade behavior reveals where a single saturation point can bring down the entire system. Map each cascade step to the SLO it impacts.

4. **Define scaling triggers with SLO linkage**
   > ðŸ’¡ *WHY: Scaling triggered by raw CPU metrics is reactive. Scaling triggered by SLO degradation signals is proactive â€” you scale before users notice.*
   For each component, define: the metric that triggers scaling, the threshold, the scaling action, and the SLO it protects.

5. **Project costs for 3, 6, and 12 months**
   > ðŸ’¡ *WHY: Capacity plans without cost projections get rejected by management. Showing cost alongside reliability makes the tradeoff explicit.*
   Estimate monthly compute cost based on your cloud pricing or bare-metal amortization. Apply growth assumptions (e.g., 15% monthly traffic growth). Include both compute costs and storage costs â€” Prometheus TSDB storage grows with retention period and cardinality. Create a table with columns: time horizon, expected traffic, required resources, estimated cost, and SLO maintained (yes/no). Highlight the point at which your current architecture requires a redesign (e.g., "at 10Ã— traffic, Raft leader becomes the bottleneck and we need sharding").

## Done when

- [ ] Resource inventory covers all components with CPU/memory/disk/network â€” *baseline for all scaling decisions*
- [ ] Load model validated against Week 18 benchmark data â€” *ensures model accuracy*
- [ ] 2Ã— headroom confirmed achievable with current architecture â€” *meets surge requirement*
- [ ] Bottleneck cascade identifies the first saturation point â€” *guides optimization priority*
- [ ] Document committed to `week-21/day5-capacity-plan.md` â€” *referenced in Week 23 architecture narrative*

## Proof

Upload or paste your capacity plan document.

**Quick self-test:**

Q: Why 2Ã— headroom instead of 1.5Ã— or 3Ã—?
**A: 2Ã— balances cost (3Ã— is wasteful) with safety (1.5Ã— gives no room for estimation error). It handles most organic traffic spikes without emergency action.**

Q: Why should scaling triggers reference SLOs rather than raw resource metrics?
**A: Because SLO-linked triggers scale in response to user impact, not infrastructure noise. CPU at 90% might be fine if latency SLOs are still met.**

Q: What is the bottleneck cascade?
**A: The ordered sequence in which components saturate under increasing load. The first to saturate determines the system's effective capacity ceiling.**
